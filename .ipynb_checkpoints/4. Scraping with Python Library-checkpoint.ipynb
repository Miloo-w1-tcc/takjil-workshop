{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "second-phrase"
   },
   "source": [
    "# Getting Information from Social Media (Twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "proof-stand"
   },
   "source": [
    "<img src=\"Images/pic1.JPG\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sexual-vault"
   },
   "source": [
    "+ **Web Crawling** merupakan suatu program/sistem/script otomatis yang dengan suatu metode tertentu melakukan scanning halaman-halaman yang ada dalam sebuah website. Web crawling menlakukan indexing dan dapat mengambil informasi-informasi pada halaman website. Hasil dari web crawling biasanya akan digunakan untuk mempelajari isi dari halaman-halaman website.\n",
    "+ **Streaming** merupakan \n",
    "+ **Web Scraping** merupakan suatu kegiatan yang dilakukan untuk mengambil informasi dari halaman website. Web scraping biasanya mengambil informasi dari HTML yang terdapat pada halaman website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prospective-orchestra"
   },
   "source": [
    "### Contoh Scraper/crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "velvet-titanium"
   },
   "source": [
    "**Official Scraper/Crawler**   : Tweepy, Scrapy\n",
    "\n",
    "**Unofficial Scraper/Crawler** : Twitterscraper, Scweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "driving-distinction"
   },
   "source": [
    "### Example Scraping\n",
    "\n",
    "Pada sesi ini, kita akan menggunakan salah satu contoh web scraper yaitu Scweet. Scweet melakukan scraping pada halaman website twitter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "split-algeria"
   },
   "source": [
    "#### 1. Import needed library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "connected-carbon"
   },
   "source": [
    "Library yang dibutuhkan untuk scraping kali ini adalah scweet dan pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9032,
     "status": "ok",
     "timestamp": 1620128986898,
     "user": {
      "displayName": "Naufal Dzaky Anwari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjUfVofiHSrNTGJZHJIPJwL0sf7lvJPIrE2d4XMAw=s64",
      "userId": "06742799176171152084"
     },
     "user_tz": -420
    },
    "id": "lI1hgalzNgvm",
    "outputId": "98c3da84-fb57-4edb-a9d0-73616e2e2579"
   },
   "outputs": [],
   "source": [
    "!pip install Scweet==1.0\n",
    "!pip install pandas==1.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1620128992832,
     "user": {
      "displayName": "Naufal Dzaky Anwari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjUfVofiHSrNTGJZHJIPJwL0sf7lvJPIrE2d4XMAw=s64",
      "userId": "06742799176171152084"
     },
     "user_tz": -420
    },
    "id": "unlike-minneapolis"
   },
   "outputs": [],
   "source": [
    "from Scweet.scweet import scrap\n",
    "from Scweet.user import get_user_information, get_users_following, get_users_followers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "random-element"
   },
   "source": [
    "#### 2. Scrape tweet with certain words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "convinced-miami"
   },
   "source": [
    "dengan menggunakan Scweet, kita dapat mengambil top tweets yang mengandung kata tertentu. caranya dengan menggunakan module scrap yang tersedia pada library Scweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 2056,
     "status": "error",
     "timestamp": 1620128998741,
     "user": {
      "displayName": "Naufal Dzaky Anwari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjUfVofiHSrNTGJZHJIPJwL0sf7lvJPIrE2d4XMAw=s64",
      "userId": "06742799176171152084"
     },
     "user_tz": -420
    },
    "id": "strong-relief",
    "outputId": "23b35e02-150f-45c0-b71b-737a5c3def31"
   },
   "outputs": [],
   "source": [
    "# keywords\n",
    "keywords = ['kurma', 'es buah', 'gorengan', 'kolak',\n",
    "            'sop buah', 'es campur', 'es dawet',\n",
    "            'es cincau', 'es kelapa muda', 'biji salak']\n",
    "\n",
    "# Date interval\n",
    "initial_date = '2021-04-28'\n",
    "finish_date = '2021-04-30'\n",
    "\n",
    "all_datas = []\n",
    "for x in keywords:\n",
    "    data = scrap(words=x,\n",
    "                 start_date=initial_date,\n",
    "                 max_date=finish_date,\n",
    "                 from_account=None,\n",
    "                 interval=1, \n",
    "                 headless=True,\n",
    "                 save_images=False,\n",
    "                 display_type=None,\n",
    "                 resume=False,\n",
    "                 filter_replies=True,\n",
    "                 proximity=True)\n",
    "    \n",
    "    data['keyword'] = x\n",
    "    all_datas.append(data)\n",
    "\n",
    "all_datas = pd.concat(all_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "polished-jenny",
    "outputId": "c22488d5-c278-4d57-a165-e3f2f90a688e"
   },
   "outputs": [],
   "source": [
    "all_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unknown-impossible",
    "outputId": "1c68b14c-6f82-44e5-b5ec-b5810c1b932a"
   },
   "outputs": [],
   "source": [
    "all_datas[all_datas['keyword'] == 'es buah']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lightweight-surfing"
   },
   "outputs": [],
   "source": [
    "# Save data to csv\n",
    "filename = 'all_keywordsv2.csv'\n",
    "all_datas.to_csv(filename, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "incorporated-chemical",
    "outputId": "670abc25-c7e3-448d-86bb-7edaeaf28648"
   },
   "outputs": [],
   "source": [
    "hashtag = 'sopbuah'\n",
    "\n",
    "initial_date = '2021-04-28'\n",
    "finish_date = '2021-04-30'\n",
    "\n",
    "data = scrap(hashtag=hashtag,\n",
    "             start_date=initial_date,\n",
    "             max_date=finish_date,\n",
    "             from_account=None,\n",
    "             interval=5,\n",
    "             headless=True,\n",
    "             display_type=\"Top\",\n",
    "             save_images=False, \n",
    "             resume=False,\n",
    "             filter_replies=False,\n",
    "             proximity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "norman-hospital",
    "outputId": "ef61011f-67d1-41c9-9702-1f5d5434490e"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swiss-spanking"
   },
   "source": [
    "### Get the main information of a given list of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amazing-bottom",
    "outputId": "89313c32-04f6-4f2a-be7e-b494ad4a80b7"
   },
   "outputs": [],
   "source": [
    "users = ['@raisa6690', '@isyanasarasvati']\n",
    "\n",
    "# this function return a list that contains : \n",
    "# [\"nb of following\",\"nb of followers\", \"join date\", \"birthdate\", \"location\", \"website\", \"description\"]\n",
    "\n",
    "users_info = get_user_information(users, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "novel-service",
    "outputId": "1cf729d8-1329-41a7-fbb5-9f2457f18e37"
   },
   "outputs": [],
   "source": [
    "users_df = pd.DataFrame(users_info, index = [\"nb of following\",\n",
    "                                             \"nb of followers\",\n",
    "                                             \"join date\", \n",
    "                                             \"birthdate\",\n",
    "                                             \"location\",\n",
    "                                             \"website\",\n",
    "                                             \"description\"]).T\n",
    "users_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Scraping with Python Library.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
